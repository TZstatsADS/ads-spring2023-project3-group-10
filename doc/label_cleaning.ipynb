{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "\n",
        "# with zipfile.ZipFile('/content/drive/MyDrive/train_dat/train_data.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/drive/MyDrive/train_dat')"
      ],
      "metadata": {
        "id": "hG0zf8Ce7CeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ud0xje3E6_CL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example image plot\n",
        "img1 = cv2.imread('/content/drive/MyDrive/train_dat/images/{:05d}.png'.format(1))\n",
        "plt.imshow(img1)\n",
        "plt.xticks([]) ## remove the ticks on x-axis\n",
        "plt.yticks([]) ## remove the ticks on y-axis\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "FwXpFR75oC_g",
        "outputId": "509283c8-f775-4635-ecf4-68a2af812935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "noisy = pd.read_csv('/content/drive/MyDrive/train_dat/noisy_labels.csv',header=None)\n",
        "clean = pd.read_csv('/content/drive/MyDrive/train_dat/clean_labels.csv',header=None)\n"
      ],
      "metadata": {
        "id": "_X0efTWGBJed"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for i in range(1,10001):\n",
        "  images.append(cv2.imread('/content/drive/MyDrive/train_dat/images/{:05d}.png'.format(i)))\n",
        "  if i % 100 == 0:\n",
        "    print(i)\n",
        "images = torch.Tensor(np.stack(images))"
      ],
      "metadata": {
        "id": "vUOD7vT9BXcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff97e07-53d8-49ca-acc9-583e3c5f57ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sz = 50\n",
        "images_trainloader = torch.utils.data.DataLoader(images[:6000], batch_size=sz)\n",
        "noisylabel_trainloader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(noisy[0].iloc[:6000].values).long()), batch_size=sz)\n",
        "label_trainloader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(clean[0].iloc[:6000].values).long()), batch_size=sz)\n",
        "\n",
        "images_valoader = torch.utils.data.DataLoader(images[6000:8000], batch_size=sz)\n",
        "noisylabel_valoader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(noisy[0].iloc[6000:8000].values).long()), batch_size=sz)\n",
        "label_valoader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(clean[0].iloc[6000:8000].values).long()), batch_size=sz)\n",
        "\n",
        "images_testloader = torch.utils.data.DataLoader(images[8000:10000], batch_size=sz)\n",
        "noisylabel_testloader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(noisy[0].iloc[8000:10000].values).long()), batch_size=sz)\n",
        "label_testloader = torch.utils.data.DataLoader(nn.functional.one_hot(torch.Tensor(clean[0].iloc[8000:10000].values).long()), batch_size=sz)\n",
        "\n"
      ],
      "metadata": {
        "id": "oXdDVALVSZ27"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nclass = 10\n",
        "class MyCNN(nn.Module):\n",
        "    def __init__(self,num_class):\n",
        "        super(MyCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8 + num_class, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_class)\n",
        "        self.sm1 = nn.Softmax(1)\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, x,y):\n",
        "        y = torch.Tensor(y)\n",
        "        \n",
        "        x = x.permute(0,3,1,2)\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = x.reshape(x.shape[0],32*8*8)\n",
        "        # print(x.shape,torch.Tensor(y).shape)\n",
        "        x = torch.cat((x, y), dim=1)\n",
        "        \n",
        "        x = x.reshape(-1, 32 * 8 * 8 + self.num_class)\n",
        "        x = self.fc1(x.float())\n",
        "        \n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sm1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = MyCNN(nclass)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# criterion = nn.L1Loss()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "piMLiNuAK-Gd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 75\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# add .float() when using crossentropyloss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (d, nl, l) in enumerate(zip(images_trainloader, noisylabel_trainloader, label_trainloader), 1):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(d, nl)\n",
        "        loss = criterion(outputs, l.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss/len(images_trainloader)))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for j, (val_d, val_nl, val_l) in enumerate(zip(images_valoader, noisylabel_valoader, label_valoader), 1):\n",
        "            val_outputs = model(val_d, val_nl)\n",
        "            val_loss += criterion(val_outputs, val_l.float()).item()\n",
        "            _, predicted = torch.max(val_outputs.data, 1)\n",
        "            _, gt = torch.max(val_l.data, 1)\n",
        "            total += val_l.size(0)\n",
        "            correct += (predicted == gt).sum().item()\n",
        "        val_loss /= len(images_valoader)\n",
        "        val_acc = 100 * correct / total\n",
        "        print('Epoch [{}/{}], Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%'.format(epoch+1, num_epochs, val_loss, val_acc))\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        epochs_since_improvement = 0\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "        \n",
        "    if epochs_since_improvement > patience:\n",
        "        print('Stopping training after {} epochs without improvement'.format(epochs_since_improvement))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nillOQagU8z",
        "outputId": "3cef7f50-6430-474b-ef62-178b5b3fc42e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/75], Training Loss: 2.1928\n",
            "Epoch [1/75], Validation Loss: 2.1688, Validation Accuracy: 30.15%\n",
            "Epoch [2/75], Training Loss: 2.1292\n",
            "Epoch [2/75], Validation Loss: 2.1342, Validation Accuracy: 32.40%\n",
            "Epoch [3/75], Training Loss: 2.0966\n",
            "Epoch [3/75], Validation Loss: 2.1158, Validation Accuracy: 33.95%\n",
            "Epoch [4/75], Training Loss: 2.0707\n",
            "Epoch [4/75], Validation Loss: 2.1027, Validation Accuracy: 35.75%\n",
            "Epoch [5/75], Training Loss: 2.0543\n",
            "Epoch [5/75], Validation Loss: 2.0867, Validation Accuracy: 37.90%\n",
            "Epoch [6/75], Training Loss: 2.0384\n",
            "Epoch [6/75], Validation Loss: 2.0769, Validation Accuracy: 38.25%\n",
            "Epoch [7/75], Training Loss: 2.0208\n",
            "Epoch [7/75], Validation Loss: 2.0785, Validation Accuracy: 37.75%\n",
            "Epoch [8/75], Training Loss: 2.0102\n",
            "Epoch [8/75], Validation Loss: 2.0722, Validation Accuracy: 38.40%\n",
            "Epoch [9/75], Training Loss: 2.0025\n",
            "Epoch [9/75], Validation Loss: 2.0619, Validation Accuracy: 39.90%\n",
            "Epoch [10/75], Training Loss: 1.9889\n",
            "Epoch [10/75], Validation Loss: 2.0615, Validation Accuracy: 40.00%\n",
            "Epoch [11/75], Training Loss: 1.9754\n",
            "Epoch [11/75], Validation Loss: 2.0570, Validation Accuracy: 40.40%\n",
            "Epoch [12/75], Training Loss: 1.9624\n",
            "Epoch [12/75], Validation Loss: 2.0450, Validation Accuracy: 41.60%\n",
            "Epoch [13/75], Training Loss: 1.9586\n",
            "Epoch [13/75], Validation Loss: 2.0412, Validation Accuracy: 41.60%\n",
            "Epoch [14/75], Training Loss: 1.9515\n",
            "Epoch [14/75], Validation Loss: 2.0331, Validation Accuracy: 42.65%\n",
            "Epoch [15/75], Training Loss: 1.9407\n",
            "Epoch [15/75], Validation Loss: 2.0252, Validation Accuracy: 43.45%\n",
            "Epoch [16/75], Training Loss: 1.9318\n",
            "Epoch [16/75], Validation Loss: 2.0204, Validation Accuracy: 43.35%\n",
            "Epoch [17/75], Training Loss: 1.9312\n",
            "Epoch [17/75], Validation Loss: 2.0271, Validation Accuracy: 43.10%\n",
            "Epoch [18/75], Training Loss: 1.9226\n",
            "Epoch [18/75], Validation Loss: 2.0158, Validation Accuracy: 44.05%\n",
            "Epoch [19/75], Training Loss: 1.9142\n",
            "Epoch [19/75], Validation Loss: 2.0223, Validation Accuracy: 43.15%\n",
            "Epoch [20/75], Training Loss: 1.9173\n",
            "Epoch [20/75], Validation Loss: 2.0249, Validation Accuracy: 43.25%\n",
            "Epoch [21/75], Training Loss: 1.9051\n",
            "Epoch [21/75], Validation Loss: 2.0212, Validation Accuracy: 43.85%\n",
            "Epoch [22/75], Training Loss: 1.8948\n",
            "Epoch [22/75], Validation Loss: 2.0218, Validation Accuracy: 43.40%\n",
            "Stopping training after 4 epochs without improvement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation on test set\n",
        "def evaluator(model,images_testloader, noisylabel_testloader, label_testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for t, (t_d, t_nl, t_l) in enumerate(zip(images_testloader, noisylabel_testloader, label_testloader), 1):\n",
        "    test_outputs = model(t_d, t_nl)\n",
        "    correct += sum(torch.argmax(test_outputs, dim=1) == torch.argmax(t_l, dim=1))\n",
        "    total += len(t_l)\n",
        "  accuracy = correct / total\n",
        "  print('Accuracy on the test set: %f' % (correct / total))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "RR7B9XEmYc3Q"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluator_th(model,threshold,it,nlt,lt):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  # set a confidence threshold, which if not passed, means you just pick the original noisy label\n",
        "  for t, (t_d, t_nl, t_l) in enumerate(zip(it, nlt, lt), 1):\n",
        "    test_outputs = model(t_d, t_nl)\n",
        "    max_vals, max_indices = torch.max(test_outputs, dim=1)\n",
        "    predictions = torch.Tensor([torch.argmax(test_outputs[i]) if max_vals[i] > threshold else torch.argmax(t_nl[i]) for i in range(len(t_nl))]).long()\n",
        "    # preds = torch.zeros_like(test_outputs).scatter_(1, torch.argmax(test_outputs, dim=1).unsqueeze(1), 1)\n",
        "    correct += sum(predictions == torch.argmax(t_l, dim=1))\n",
        "    total += len(t_l)\n",
        "  accuracy = correct / total\n",
        "  print('Accuracy on the test set: %f with threshold %f' % (correct / total, threshold))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "o1te-klNgQDQ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testacc = evaluator(model,images_testloader, noisylabel_testloader, label_testloader)\n",
        "testacc2 = evaluator_th(model,0.9,images_testloader, noisylabel_testloader, label_testloader)"
      ],
      "metadata": {
        "id": "yD8Q4kMng20P"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark\n",
        "np.mean(noisy[0][:10000] == clean[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbBisD_cio2K",
        "outputId": "380ca4b7-e353-4a7b-f6d4-9266da255884"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3968"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A slightly more complex network than the base\n",
        "nclass = 10\n",
        "class MyCNN2(nn.Module):\n",
        "    def __init__(self,num_class):\n",
        "        super(MyCNN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8 + num_class, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_class)\n",
        "        self.sm1 = nn.Softmax(1)\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, x,y):\n",
        "        y = torch.Tensor(y)\n",
        "        \n",
        "        x = x.permute(0,3,1,2)\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = x.reshape(x.shape[0],64*8*8)\n",
        "        # print(x.shape,torch.Tensor(y).shape)\n",
        "        x = torch.cat((x, y), dim=1)\n",
        "        \n",
        "        x = x.reshape(-1, 64 * 8 * 8 + self.num_class)\n",
        "        x = self.fc1(x.float())\n",
        "        \n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sm1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = MyCNN2(nclass)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "e9Me2J-pH4Du"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 75\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# add .float() when using crossentropyloss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (d, nl, l) in enumerate(zip(images_trainloader, noisylabel_trainloader, label_trainloader), 1):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(d, nl)\n",
        "        loss = criterion(outputs, l.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch [{}/{}], Training Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss/len(images_trainloader)))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for j, (val_d, val_nl, val_l) in enumerate(zip(images_valoader, noisylabel_valoader, label_valoader), 1):\n",
        "            val_outputs = model(val_d, val_nl)\n",
        "            val_loss += criterion(val_outputs, val_l.float()).item()\n",
        "            _, predicted = torch.max(val_outputs.data, 1)\n",
        "            _, gt = torch.max(val_l.data, 1)\n",
        "            total += val_l.size(0)\n",
        "            correct += (predicted == gt).sum().item()\n",
        "        val_loss /= len(images_valoader)\n",
        "        val_acc = 100 * correct / total\n",
        "        print('Epoch [{}/{}], Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%'.format(epoch+1, num_epochs, val_loss, val_acc))\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        epochs_since_improvement = 0\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "        \n",
        "    if epochs_since_improvement > patience:\n",
        "        print('Stopping training after {} epochs without improvement'.format(epochs_since_improvement))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccOwUCkChv0g",
        "outputId": "db7fe5a9-81d7-40bb-abe5-f3cef78ab0c8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/75], Training Loss: 2.2688\n",
            "Epoch [1/75], Validation Loss: 2.2435, Validation Accuracy: 26.85%\n",
            "Epoch [2/75], Training Loss: 2.1562\n",
            "Epoch [2/75], Validation Loss: 2.1383, Validation Accuracy: 33.60%\n",
            "Epoch [3/75], Training Loss: 2.0867\n",
            "Epoch [3/75], Validation Loss: 2.0910, Validation Accuracy: 37.90%\n",
            "Epoch [4/75], Training Loss: 2.0505\n",
            "Epoch [4/75], Validation Loss: 2.0645, Validation Accuracy: 40.75%\n",
            "Epoch [5/75], Training Loss: 2.0213\n",
            "Epoch [5/75], Validation Loss: 2.0454, Validation Accuracy: 42.20%\n",
            "Epoch [6/75], Training Loss: 2.0013\n",
            "Epoch [6/75], Validation Loss: 2.0343, Validation Accuracy: 42.25%\n",
            "Epoch [7/75], Training Loss: 1.9818\n",
            "Epoch [7/75], Validation Loss: 2.0250, Validation Accuracy: 43.65%\n",
            "Epoch [8/75], Training Loss: 1.9637\n",
            "Epoch [8/75], Validation Loss: 2.0142, Validation Accuracy: 44.80%\n",
            "Epoch [9/75], Training Loss: 1.9492\n",
            "Epoch [9/75], Validation Loss: 2.0123, Validation Accuracy: 45.15%\n",
            "Epoch [10/75], Training Loss: 1.9366\n",
            "Epoch [10/75], Validation Loss: 2.0074, Validation Accuracy: 45.35%\n",
            "Epoch [11/75], Training Loss: 1.9281\n",
            "Epoch [11/75], Validation Loss: 2.0006, Validation Accuracy: 45.55%\n",
            "Epoch [12/75], Training Loss: 1.9216\n",
            "Epoch [12/75], Validation Loss: 1.9991, Validation Accuracy: 46.25%\n",
            "Epoch [13/75], Training Loss: 1.9144\n",
            "Epoch [13/75], Validation Loss: 2.0077, Validation Accuracy: 44.60%\n",
            "Epoch [14/75], Training Loss: 1.9024\n",
            "Epoch [14/75], Validation Loss: 1.9997, Validation Accuracy: 46.25%\n",
            "Epoch [15/75], Training Loss: 1.8894\n",
            "Epoch [15/75], Validation Loss: 1.9843, Validation Accuracy: 47.60%\n",
            "Epoch [16/75], Training Loss: 1.8751\n",
            "Epoch [16/75], Validation Loss: 1.9780, Validation Accuracy: 48.90%\n",
            "Epoch [17/75], Training Loss: 1.8625\n",
            "Epoch [17/75], Validation Loss: 1.9687, Validation Accuracy: 49.65%\n",
            "Epoch [18/75], Training Loss: 1.8482\n",
            "Epoch [18/75], Validation Loss: 1.9663, Validation Accuracy: 49.20%\n",
            "Epoch [19/75], Training Loss: 1.8360\n",
            "Epoch [19/75], Validation Loss: 1.9604, Validation Accuracy: 49.70%\n",
            "Epoch [20/75], Training Loss: 1.8292\n",
            "Epoch [20/75], Validation Loss: 1.9624, Validation Accuracy: 50.15%\n",
            "Epoch [21/75], Training Loss: 1.8227\n",
            "Epoch [21/75], Validation Loss: 1.9636, Validation Accuracy: 50.05%\n",
            "Epoch [22/75], Training Loss: 1.8053\n",
            "Epoch [22/75], Validation Loss: 1.9576, Validation Accuracy: 50.85%\n",
            "Epoch [23/75], Training Loss: 1.7965\n",
            "Epoch [23/75], Validation Loss: 1.9601, Validation Accuracy: 50.90%\n",
            "Epoch [24/75], Training Loss: 1.7834\n",
            "Epoch [24/75], Validation Loss: 1.9538, Validation Accuracy: 51.20%\n",
            "Epoch [25/75], Training Loss: 1.7726\n",
            "Epoch [25/75], Validation Loss: 1.9477, Validation Accuracy: 51.15%\n",
            "Epoch [26/75], Training Loss: 1.7711\n",
            "Epoch [26/75], Validation Loss: 1.9456, Validation Accuracy: 51.35%\n",
            "Epoch [27/75], Training Loss: 1.7654\n",
            "Epoch [27/75], Validation Loss: 1.9563, Validation Accuracy: 50.30%\n",
            "Epoch [28/75], Training Loss: 1.7580\n",
            "Epoch [28/75], Validation Loss: 1.9666, Validation Accuracy: 48.60%\n",
            "Epoch [29/75], Training Loss: 1.7486\n",
            "Epoch [29/75], Validation Loss: 1.9631, Validation Accuracy: 49.50%\n",
            "Epoch [30/75], Training Loss: 1.7398\n",
            "Epoch [30/75], Validation Loss: 1.9503, Validation Accuracy: 50.20%\n",
            "Stopping training after 4 epochs without improvement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testacc = evaluator(model,images_testloader, noisylabel_testloader, label_testloader)\n",
        "testacc2 = evaluator_th(model,0.9,images_testloader, noisylabel_testloader, label_testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZtC-5PIhzg7",
        "outputId": "2f63641b-2f99-422c-b671-0358e3c920c5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.520000\n",
            "Accuracy on the test set: 0.531000 with threshold 0.900000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data to test the network\n",
        "\n",
        "#noisylabel_loader\n",
        "# nn.functional.one_hot(torch.Tensor(noisy[0].iloc[:10000].values).long())\n",
        "#label_loader\n",
        "# nn.functional.one_hot(torch.Tensor(clean[0].values).long())\n",
        "\n",
        "import random\n",
        "# batch size 4\n",
        "sz = 4\n",
        "trainloader = torch.utils.data.DataLoader(torch.Tensor(np.random.rand(8,32,32,3)), batch_size=sz)\n",
        "# number of classes 10\n",
        "nclass = 10\n",
        "\n",
        "ys = np.zeros(shape=(8,nclass))\n",
        "for i in range(ys.shape[0]):\n",
        "  labs = np.zeros(nclass)\n",
        "  labs[random.randint(0,nclass-1)] = 1\n",
        "  ys[i] = labs\n",
        "noisylabel_loader = torch.utils.data.DataLoader(ys, batch_size=sz)\n",
        "\n",
        "ys = np.zeros(shape=(8,nclass))\n",
        "for i in range(ys.shape[0]):\n",
        "  labs = np.zeros(nclass)\n",
        "  labs[random.randint(0,nclass-1)] = 1\n",
        "  ys[i] = labs\n",
        "label_loader = torch.utils.data.DataLoader(ys, batch_size=sz)\n",
        "\n",
        "for i, (d, nl, l) in enumerate(zip(trainloader,noisylabel_loader,label_loader),0):\n",
        "  print(i,d.shape,nl.shape,l.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkFhSjE3RUEo",
        "outputId": "f49721be-3ab3-4627-c4d6-f1fa395db97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([4, 32, 32, 3]) torch.Size([4, 10]) torch.Size([4, 10])\n",
            "1 torch.Size([4, 32, 32, 3]) torch.Size([4, 10]) torch.Size([4, 10])\n"
          ]
        }
      ]
    }
  ]
}